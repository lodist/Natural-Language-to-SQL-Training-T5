{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "file_path = 'Sample_Training_Data.csv'\n",
    "df = pd.read_csv(file_path, sep=';', header=None, names=['Input', 'Value'])\n",
    "\n",
    "# Drop the first row (header row in data)\n",
    "df = df.drop(0)\n",
    "\n",
    "# Combine input and value into one sequence for training\n",
    "df['text'] = df['Input'].str.strip() + \" ; \" + df['Value'].str.strip()\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['Input', 'Value']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['Input', 'Value']])\n",
    "\n",
    "# Load pre-trained T5 tokenizer and model\n",
    "model_name = \"t5-base\"  # t-small \"t5-base\" or \"t5-large\" for larger models\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = [\"translate Input to Output: \" + inp for inp in examples[\"Input\"]]\n",
    "    outputs = [outp for outp in examples[\"Value\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=250, truncation=True, padding=\"max_length\")  # Max length = 250\n",
    "    labels = tokenizer(outputs, max_length=250, truncation=True, padding=\"max_length\")      # Max length = 250\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True, remove_columns=['Input', 'Value'])\n",
    "val_tokenized = val_dataset.map(tokenize_function, batched=True, remove_columns=['Input', 'Value'])\n",
    "\n",
    "# Load pre-trained T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",                       # Changed to match the current strategy\n",
    "    save_strategy=\"epoch\",                       # Save at each epoch\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,              # Larger batch size if memory allows\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=30,                         # More epochs for better convergence\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=500                               # Keeping this as per your preference\n",
    ")\n",
    "\n",
    "# Initialize Trainer with training and validation datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,  # Use the training dataset\n",
    "    eval_dataset=val_tokenized      # Use the validation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./trained_model_t5_V2')\n",
    "tokenizer.save_pretrained('./trained_model_t5_V2')\n",
    "\n",
    "# Example function to generate a response based on user input\n",
    "def generate_response(input_text):\n",
    "    input_text = \"translate Input to Output: \" + input_text.strip()\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=250, truncation=True)  # Max length = 250\n",
    "    outputs = model.generate(**inputs, max_length=150, num_beams=2, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_input = \"EInschluss: Alter zw. 26 und 80\"\n",
    "response = generate_response(test_input)\n",
    "print(\"Generated Output:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the token length of the longest input-output pair\n",
    "longest_example = \"Einschluss: Produkt Mitgliedschaft mit Pannenhilfe, Motiv unbezahlt, Kündigungsdatum : 01.06.2022 – 31.03.2023;isIncludedWithDate(listProducts_Benificiary_latestCancellationReasons, '101-breakdownVariant-MOT','2022-06-01') and isIncludedWithDate(listProducts_Benificiary_latestCancellationReasons, '101-cancellationReason-motif54,101-cancellationReason-motif16','2022-06-01') and !isIncludedWithDate(listProducts_Benificiary_latestCancellationReasons, '101-breakdownVariant-MOT','2023-03-31')\"\n",
    "\n",
    "# Tokenize and print the token length\n",
    "tokenized_example = tokenizer(longest_example, return_tensors=\"pt\")\n",
    "print(f\"Token length of the longest example: {len(tokenized_example['input_ids'][0])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
